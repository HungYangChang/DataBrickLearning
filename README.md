# Databricks + Apache Iceberg Learning Project

> A hands-on guide to mastering Apache Iceberg on Databricks through building a Slowly Changing Dimension (SCD) pipeline.

---

## Quick Start

1. **New to Iceberg?** â†’ Start with [Core Concepts](docs/01-iceberg-concepts.md)
2. **Ready to code?** â†’ Set up [Databricks Environment](docs/02-databricks-setup.md)
3. **Run the notebooks** â†’ See [How to Run](#how-to-run-the-notebooks) below
4. **Have issues?** â†’ Check the [Troubleshooting Guide](docs/08-troubleshooting.md)

---

## What You'll Build

A **product catalog pipeline** that demonstrates every important Iceberg feature:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PRODUCT CATALOG PIPELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   ğŸ“¦ Daily Product Feed                                             â”‚
â”‚         â”‚                                                           â”‚
â”‚         â–¼                                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚              ICEBERG TABLE                               â”‚      â”‚
â”‚   â”‚                                                          â”‚      â”‚
â”‚   â”‚   â€¢ Full history tracking (SCD Type 2)                  â”‚      â”‚
â”‚   â”‚   â€¢ Query any point in time                             â”‚      â”‚
â”‚   â”‚   â€¢ Schema changes without rewrite                      â”‚      â”‚
â”‚   â”‚   â€¢ Read from any engine                                â”‚      â”‚
â”‚   â”‚                                                          â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                                                           â”‚
â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚         â–¼               â–¼               â–¼                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚Databricksâ”‚   â”‚  Trino   â”‚   â”‚ Athena   â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Documentation

### Core Learning

| Document | Description | Time |
|----------|-------------|------|
| [01 - Iceberg Concepts](docs/01-iceberg-concepts.md) | Metadata architecture, snapshots, partitioning | 20-30 min |
| [02 - Databricks Setup](docs/02-databricks-setup.md) | Environment setup, cluster config, verification | 30-60 min |

### Implementation Phases

| Phase | Document | What You'll Learn |
|-------|----------|-------------------|
| 1 | [Table Creation](docs/03-phase1-table-creation.md) | Schema design, partitioning, first data load |
| 2 | [Merge Operations](docs/04-phase2-merge-operations.md) | MERGE INTO, SCD Type 2 logic, COW vs MOR |
| 3 | [Time Travel](docs/05-phase3-time-travel.md) | Historical queries, auditing, snapshot management |
| 4 | [Schema Evolution](docs/06-phase4-evolution.md) | Add columns, change partitions, no rewrite |
| 5 | [Multi-Engine Access](docs/07-phase5-multi-engine.md) | Trino, Spark, Athena reading same data |

### Reference

| Document | Description |
|----------|-------------|
| [Troubleshooting](docs/08-troubleshooting.md) | Common issues and solutions |

---

## Learning Path

```
Week 1: Foundations
â”œâ”€â”€ Day 1-2: Read Iceberg Concepts (docs/01)
â”œâ”€â”€ Day 3-4: Setup Databricks (docs/02)
â””â”€â”€ Day 5-7: Complete Phase 1 (docs/03)

Week 2: Core Operations  
â”œâ”€â”€ Day 1-3: Complete Phase 2 - Merge (docs/04)
â””â”€â”€ Day 4-7: Complete Phase 3 - Time Travel (docs/05)

Week 3: Advanced Features
â”œâ”€â”€ Day 1-3: Complete Phase 4 - Evolution (docs/06)
â””â”€â”€ Day 4-7: Complete Phase 5 - Multi-Engine (docs/07)
```

---

## Local Development Setup

The notebooks run on **Databricks**, but a local virtual environment gives you IDE autocomplete, linting, and access to the Databricks CLI.

```bash
# 1. Clone the repo
git clone <repo-url> && cd DataBrickLearning

# 2. Create a virtual environment
python3 -m venv .venv

# 3. Activate it
source .venv/bin/activate        # macOS / Linux
# .venv\Scripts\activate         # Windows

# 4. Install dependencies
pip install -r requirements.txt
```

**What gets installed:**

| Package | Purpose |
|---------|---------|
| `pyspark` | IDE autocomplete & type-checking for Spark code |
| `ruff` | Fast Python linter & formatter |
| `databricks-cli` | Import/export notebooks, manage clusters from terminal |

---

## How to Run the Notebooks

> **Important:** These are Databricks-format notebooks (`.py` with `# MAGIC` cells).
> They **must** run on a Databricks cluster â€” they cannot be executed locally with `python`.

### Option A: Import via Databricks UI (Recommended)

1. Open your Databricks workspace
2. Navigate to **Workspace** â†’ **Users** â†’ your user folder
3. Click **Import** (top-right dropdown)
4. Choose **File** and select the `.py` notebook (e.g., `notebooks/01-setup-and-initial-load.py`)
5. Attach to your `iceberg-learning` cluster
6. Run cells with **Shift + Enter**

### Option B: Import via Databricks CLI

```bash
# One-time setup: configure authentication
databricks configure --token
#   Host: https://<your-workspace>.cloud.databricks.com
#   Token: <your personal access token>

# Import a single notebook
databricks workspace import \
  notebooks/01-setup-and-initial-load.py \
  /Users/<your-email>/iceberg-learning/01-setup-and-initial-load \
  --language PYTHON --overwrite

# Or import the entire notebooks/ folder
databricks workspace import_dir \
  notebooks/ \
  /Users/<your-email>/iceberg-learning \
  --overwrite
```

### Option C: Databricks Repos (Git Integration)

1. Push this repo to GitHub / GitLab / etc.
2. In Databricks, go to **Repos** â†’ **Add Repo**
3. Paste the repo URL
4. Open notebooks directly from the Repos UI â€” changes sync with Git

---

## Project Structure

```
DataBrickLearning/
â”‚
â”œâ”€â”€ README.md                          â† You are here
â”œâ”€â”€ requirements.txt                   â† Python dependencies for local dev
â”œâ”€â”€ .gitignore                         â† Excludes .venv, __pycache__, etc.
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 01-iceberg-concepts.md         â† Core concepts explained
â”‚   â”œâ”€â”€ 02-databricks-setup.md         â† Environment setup guide
â”‚   â”œâ”€â”€ 03-phase1-table-creation.md    â† Create first Iceberg table
â”‚   â”œâ”€â”€ 04-phase2-merge-operations.md  â† Implement MERGE/upsert
â”‚   â”œâ”€â”€ 05-phase3-time-travel.md       â† Time travel & auditing
â”‚   â”œâ”€â”€ 06-phase4-evolution.md         â† Schema & partition changes
â”‚   â”œâ”€â”€ 07-phase5-multi-engine.md      â† Multi-engine access
â”‚   â””â”€â”€ 08-troubleshooting.md          â† Problem solving guide
â”‚
â”œâ”€â”€ notebooks/                         â† Databricks notebooks (.py source format)
â”‚   â”œâ”€â”€ 01-setup-and-initial-load.py   â† Phase 1: Create table, load 1K products
â”‚   â”œâ”€â”€ 02-merge-operations.py         â† (Phase 2: coming next)
â”‚   â”œâ”€â”€ 03-time-travel-audit.py        â† (Phase 3)
â”‚   â”œâ”€â”€ 04-schema-evolution.py         â† (Phase 4)
â”‚   â””â”€â”€ 05-multi-engine-test.py        â† (Phase 5)
â”‚
â””â”€â”€ sample-data/                       â† (Optional: store test data)
    â””â”€â”€ products_sample.csv
```

---

## Key Concepts at a Glance

### Why Iceberg?

| Traditional Data Lake Problem | Iceberg Solution |
|------------------------------|------------------|
| No ACID transactions | Full ACID support |
| Schema changes require rewrite | Metadata-only changes |
| No time travel | Query any past state |
| Partition changes are painful | Seamless partition evolution |
| Vendor lock-in | Any engine can read/write |

### The Metadata Hierarchy

```
CATALOG          â†’ "Where is my table?"
    â”‚
METADATA FILE    â†’ "What's the current schema and snapshot?"
    â”‚
SNAPSHOT         â†’ "What was the table state at this moment?"
    â”‚
MANIFEST LIST    â†’ "Which manifest files make up this snapshot?"
    â”‚
MANIFEST FILES   â†’ "Which data files contain my data?"
    â”‚
DATA FILES       â†’ The actual Parquet files with your data
```

### SCD Type 2 Pattern

```sql
-- Every product has full history:
product_id | price  | valid_from | valid_to   | is_current
-----------|--------|------------|------------|------------
1001       | 19.99  | 2024-01-01 | 2024-01-15 | false      â† Old version
1001       | 24.99  | 2024-01-15 | NULL       | true       â† Current
```

---

## Prerequisites

- **Required:**
  - Python 3.9+ (for local virtual environment)
  - Basic SQL knowledge
  - Databricks account (free Community Edition works)
  
- **Helpful:**
  - Python/PySpark familiarity
  - Understanding of data warehousing concepts
  - Experience with cloud storage (S3, ADLS, or GCS)

---

## Quick Commands Reference

### Iceberg Table Operations

```sql
-- Create table
CREATE TABLE t USING ICEBERG PARTITIONED BY (col) ...

-- View metadata
SELECT * FROM t.snapshots;
SELECT * FROM t.history;
SELECT * FROM t.files;

-- Time travel
SELECT * FROM t VERSION AS OF <snapshot_id>;
SELECT * FROM t TIMESTAMP AS OF '2024-01-15';

-- Schema evolution
ALTER TABLE t ADD COLUMN new_col INT;
ALTER TABLE t ADD PARTITION FIELD day(ts);

-- Maintenance
OPTIMIZE t;  -- Compaction
CALL system.expire_snapshots('t', older_than => ...);
```

---

## Resources

### Official Documentation
- [Apache Iceberg Specification](https://iceberg.apache.org/spec/)
- [Databricks Iceberg Docs](https://docs.databricks.com/en/delta/iceberg.html)
- [Iceberg Spark Integration](https://iceberg.apache.org/docs/latest/spark-getting-started/)

### Community
- [Apache Iceberg Slack](https://apache-iceberg.slack.com/)
- [Databricks Community](https://community.databricks.com/)

---

## License

This learning project is for educational purposes. Feel free to use and modify for your own learning.

---

**Ready to start?** â†’ [Begin with Iceberg Concepts](docs/01-iceberg-concepts.md)
